{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import anndata as ad\n",
    "import scanpy.external as sce\n",
    "from sklearn import preprocessing\n",
    "import pickle5 as pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "eps=1e-100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f22dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torch.nn as nn\n",
    "class NNTransfer(nn.Module):\n",
    "    def __init__(self, input_dim=128, output_dim=10):\n",
    "        super(NNTransfer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "        self.activate = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x= self.activate(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "def NNTransferTrain(model, criterion, optimizer, train_loader,val_loader, device, \n",
    "                    save_pth=None, epochs=200):\n",
    "    eval_accuracy_mini=0#np.inf\n",
    "    patience_count=0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_all=0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_all+=loss.item()\n",
    "        eval_loss, eval_accuracy = NNTransferEvaluate(model, val_loader, criterion, device)\n",
    "        if eval_accuracy_mini<eval_accuracy:\n",
    "            eval_accuracy_mini=eval_accuracy\n",
    "#             torch.save(model.state_dict(), save_pth)\n",
    "            print(f\"Epoch {epoch}/{epochs} - Train Loss: {loss_all / len(train_loader)}, Accuracy: {eval_accuracy}\")\n",
    "            patience_count=0\n",
    "        else:\n",
    "            patience_count+=1\n",
    "        if patience_count>10:\n",
    "            p=0\n",
    "            print(f\"Epoch {epoch}/{epochs} - early stopping due to patience count\")\n",
    "            break\n",
    "            \n",
    "def NNTransferEvaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    accuracy = 100. * correct / total\n",
    "    return total_loss/len(dataloader), accuracy\n",
    "\n",
    "def NNTransferPredictWithUncertainty(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_uncertainties = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs in dataloader:\n",
    "            inputs = inputs[0].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            confidence = torch.max(outputs, dim=1)[0]\n",
    "            uncertainty = 1 - confidence\n",
    "            all_predictions.extend(predicted.detach().cpu().numpy())\n",
    "            all_uncertainties.extend(uncertainty.detach().cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_predictions), np.vstack(all_uncertainties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72426e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "starmap_adata_obs = pd.read_csv('source_data/color/starmap_sub_old.csv',index_col=0)\n",
    "color_dic = dict(zip(starmap_adata_obs['key'],\n",
    "                     starmap_adata_obs['color']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7592cc23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tissueregion_starmap=sc.read_h5ad('source_data/ad_embed.h5ad')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7e311",
   "metadata": {},
   "source": [
    "### Transfer A1N main level labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d19be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_embed_train = tissueregion_starmap[tissueregion_starmap.obs.loc[tissueregion_starmap.obs['gt_tissue_region_main']!='NA'].index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885dbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1_embeddings = ad_embed_train.X\n",
    "sample1_labels = list(ad_embed_train.obs['gt_tissue_region_main'])\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(sample1_labels)\n",
    "\n",
    "\n",
    "sample1_labels = le.transform(sample1_labels)\n",
    "sample1_labels = sample1_labels.astype('str').astype('int')\n",
    "\n",
    "\n",
    "dataset1 = TensorDataset(torch.Tensor(sample1_embeddings), torch.Tensor(sample1_labels).long())\n",
    "train_size = int(0.8 * len(dataset1))  # Use 80% of the data for training\n",
    "val_size = len(dataset1) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset1, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(0.5 * len(val_dataset))  # Use 10% of the data for val and 10% for testing \n",
    "test_size = len(val_dataset) - val_size\n",
    "val_dataset, test_dataset = random_split(val_dataset, [val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e366a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weight = torch.Tensor(sklearn.utils.class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                            classes=np.unique(sample1_labels),\n",
    "                                                                            y=sample1_labels))\n",
    "model = NNTransfer(input_dim=sample1_embeddings.shape[1],\n",
    "                   output_dim=len(np.unique(sample1_labels)))\n",
    "model.to(device)  # Move the model to GPU if available\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weight.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "NNTransferTrain(model, criterion, optimizer, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a81a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions,sample2_uncertainty = NNTransferPredictWithUncertainty(model, test_loader, device)\n",
    "test_predictions = le.inverse_transform(test_predictions)\n",
    "\n",
    "all_labels = [label.item() for _, label in test_dataset]\n",
    "\n",
    "\n",
    "gt_test_predictions = le.inverse_transform(all_labels)\n",
    "\n",
    "GT_starmap_s = gt_test_predictions\n",
    "PRED_starmap_s = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8401c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab = pd.crosstab(pd.Series(GT_starmap_s, name='Original'),\n",
    "                                pd.Series(PRED_starmap_s, name='FuseMap'))\n",
    "\n",
    "cross_tab_normalized = cross_tab.div(cross_tab.sum(axis=0), axis=1)\n",
    "cross_tab_normalized = cross_tab_normalized.div(cross_tab_normalized.sum(axis=1), axis=0)\n",
    "\n",
    "cross_tab_normalized = cross_tab_normalized*100\n",
    "cross_tab_normalized = np.around(cross_tab_normalized)\n",
    "cross_tab_normalized=cross_tab_normalized.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b43ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap\n",
    "cmap = sns.cubehelix_palette(start=2, rot=0, dark=0, light=1.05, reverse=False, as_cmap=True)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "ax=sns.heatmap(cross_tab_normalized, cmap=cmap,)\n",
    "plt.title(\"Normalized Correspondence of Two Categories\")\n",
    "# plt.savefig('figures_refine/main_ct_starmap.png',dpi=300, transparent=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
