{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2ae124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import anndata as ad\n",
    "import scanpy.external as sce\n",
    "from sklearn import preprocessing\n",
    "import pickle5 as pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "eps=1e-100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed03538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torch.nn as nn\n",
    "class NNTransfer(nn.Module):\n",
    "    def __init__(self, input_dim=128, output_dim=10):\n",
    "        super(NNTransfer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "        self.activate = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x= self.activate(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "def NNTransferTrain(model, criterion, optimizer, train_loader,val_loader, device, \n",
    "                    save_pth=None, epochs=200):\n",
    "    eval_accuracy_mini=0#np.inf\n",
    "    patience_count=0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_all=0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_all+=loss.item()\n",
    "        eval_loss, eval_accuracy = NNTransferEvaluate(model, val_loader, criterion, device)\n",
    "        if eval_accuracy_mini<eval_accuracy:\n",
    "            eval_accuracy_mini=eval_accuracy\n",
    "#             torch.save(model.state_dict(), save_pth)\n",
    "            print(f\"Epoch {epoch}/{epochs} - Train Loss: {loss_all / len(train_loader)}, Accuracy: {eval_accuracy}\")\n",
    "            patience_count=0\n",
    "        else:\n",
    "            patience_count+=1\n",
    "        if patience_count>10:\n",
    "            p=0\n",
    "            print(f\"Epoch {epoch}/{epochs} - early stopping due to patience count\")\n",
    "            break\n",
    "            \n",
    "def NNTransferEvaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    accuracy = 100. * correct / total\n",
    "    return total_loss/len(dataloader), accuracy\n",
    "\n",
    "def NNTransferPredictWithUncertainty(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_uncertainties = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs in dataloader:\n",
    "            inputs = inputs[0].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            confidence = torch.max(outputs, dim=1)[0]\n",
    "            uncertainty = 1 - confidence\n",
    "            all_predictions.extend(predicted.detach().cpu().numpy())\n",
    "            all_uncertainties.extend(uncertainty.detach().cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_predictions), np.vstack(all_uncertainties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b2bbe",
   "metadata": {},
   "source": [
    "### Read universla single-cell embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08270a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_fusemap_emb = sc.read_h5ad('source_data/ad_embed.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11301be",
   "metadata": {},
   "source": [
    "### Transfer A1N main level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87649ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_embed_train = ad_fusemap_emb[ad_fusemap_emb.obs.loc[ad_fusemap_emb.obs['gt_celltype_main_STARmap']!='nan'].index]\n",
    "ad_embed_train = ad_embed_train[ad_embed_train.obs['gt_celltype_main_STARmap']!='Unannotated',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff851456",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample1_embeddings = ad_embed_train.X\n",
    "sample1_labels = list(ad_embed_train.obs['gt_celltype_main_STARmap'])\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(sample1_labels)\n",
    "\n",
    "\n",
    "sample1_labels = le.transform(sample1_labels)\n",
    "sample1_labels = sample1_labels.astype('str').astype('int')\n",
    "\n",
    "\n",
    "dataset1 = TensorDataset(torch.Tensor(sample1_embeddings), torch.Tensor(sample1_labels).long())\n",
    "train_size = int(0.8 * len(dataset1))  # Use 80% of the data for training\n",
    "val_size = len(dataset1) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset1, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_size = int(0.5 * len(val_dataset))  # Use 10% of the data for val and 10% for testing \n",
    "test_size = len(val_dataset) - val_size\n",
    "val_dataset, test_dataset = random_split(val_dataset, [val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70238874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weight = torch.Tensor(sklearn.utils.class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                            classes=np.unique(sample1_labels),\n",
    "                                                                            y=sample1_labels))\n",
    "model = NNTransfer(input_dim=sample1_embeddings.shape[1],\n",
    "                   output_dim=len(np.unique(sample1_labels)))\n",
    "model.to(device)  # Move the model to GPU if available\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weight.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "NNTransferTrain(model, criterion, optimizer, train_loader, val_loader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d668a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions,sample2_uncertainty = NNTransferPredictWithUncertainty(model, test_loader, device)\n",
    "test_predictions = le.inverse_transform(test_predictions)\n",
    "\n",
    "all_labels = [label.item() for _, label in test_dataset]\n",
    "\n",
    "\n",
    "gt_test_predictions = le.inverse_transform(all_labels)\n",
    "\n",
    "GT_starmap_s = gt_test_predictions\n",
    "PRED_starmap_s = test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e27a9d",
   "metadata": {},
   "source": [
    "### plot the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f5c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab = pd.crosstab(pd.Series(GT_starmap_s, name='Original'),\n",
    "                                pd.Series(PRED_starmap_s, name='FuseMap'))\n",
    "\n",
    "cross_tab_normalized = cross_tab.div(cross_tab.sum(axis=0), axis=1)\n",
    "cross_tab_normalized = cross_tab_normalized.div(cross_tab_normalized.sum(axis=1), axis=0)\n",
    "\n",
    "cross_tab_normalized = cross_tab_normalized*100\n",
    "cross_tab_normalized = np.around(cross_tab_normalized)\n",
    "cross_tab_normalized=cross_tab_normalized.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e26b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_list=['Telencephalon projecting excitatory neurons','Di- and mesencephalon excitatory neurons',\n",
    "           'Telencephalon projecting inhibitory neurons', \n",
    "          'Telencephalon inhibitory interneurons','Di- and mesencephalon inhibitory neurons', \n",
    "          'Peptidergic neurons', 'Glutamatergic neuroblasts',\n",
    "          'Dentate gyrus granule neurons','Non-glutamatergic neuroblasts',\n",
    "          'Olfactory inhibitory neurons', 'Cerebellum neurons', 'Cholinergic and monoaminergic neurons',\n",
    "          'Hindbrain neurons/Spinal cord neurons','Subcommissural organ hypendymal cells','Ependymal cells', \n",
    "          'Choroid plexus epithelial cells','Astrocytes', 'Vascular smooth muscle cells' , 'Pericytes',   \n",
    "           'Vascular endothelial cells', 'Vascular and leptomeningeal cells',\n",
    "          'Perivascular macrophages', 'Microglia', 'Oligodendrocyte precursor cells', \n",
    "           'Oligodendrocytes','Olfactory ensheathing cells',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c73fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_normalized = cross_tab_normalized[sort_list]\n",
    "\n",
    "cross_tab_normalized = cross_tab_normalized.loc[sort_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d057eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cmap = sns.cubehelix_palette(start=2, rot=0, dark=0, light=1.05, reverse=False, as_cmap=True)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(7,6))\n",
    "ax=sns.heatmap(cross_tab_normalized, cmap=cmap,)\n",
    "plt.title(\"Normalized Correspondence of Two Categories\")\n",
    "# plt.savefig('figures_refine/main_ct_starmap.png',dpi=300, transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97513cf2",
   "metadata": {},
   "source": [
    "transfer to all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a86138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample2_embeddings = ad_fusemap_emb.X\n",
    "dataset2 = TensorDataset(torch.Tensor(sample2_embeddings))\n",
    "dataloader2 = DataLoader(dataset2, batch_size=256, shuffle=False)\n",
    "sample2_predictions,sample2_uncertainty = NNTransferPredictWithUncertainty(model, dataloader2, device)\n",
    "sample2_predictions = le.inverse_transform(sample2_predictions)\n",
    "\n",
    "ad_fusemap_emb.obs['transfer_gt_cell_type_main_STARmap'] = sample2_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8695e",
   "metadata": {},
   "source": [
    "### Transfer A2N main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fce03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_fusemap_emb.obs['gt_celltype_class_allen'] = ad_fusemap_emb.obs['gt_celltype_class_allen'].astype('str')\n",
    "ad_embed_train = ad_fusemap_emb[ad_fusemap_emb.obs.loc[ad_fusemap_emb.obs['gt_celltype_class_allen']!='nan'].index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429d6dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample1_embeddings = ad_embed_train.X\n",
    "sample1_labels = list(ad_embed_train.obs['gt_celltype_class_allen'])\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(sample1_labels)\n",
    "\n",
    "\n",
    "sample1_labels = le.transform(sample1_labels)\n",
    "sample1_labels = sample1_labels.astype('str').astype('int')\n",
    "\n",
    "\n",
    "dataset1 = TensorDataset(torch.Tensor(sample1_embeddings), torch.Tensor(sample1_labels).long())\n",
    "train_size = int(0.8 * len(dataset1))  # Use 80% of the data for training\n",
    "val_size = len(dataset1) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset1, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d684644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_size = int(0.5 * len(val_dataset))  # Use 10% of the data for val and 10% for testing \n",
    "test_size = len(val_dataset) - val_size\n",
    "val_dataset, test_dataset = random_split(val_dataset, [val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c4538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weight = torch.Tensor(sklearn.utils.class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                                            classes=np.unique(sample1_labels),\n",
    "                                                                            y=sample1_labels))\n",
    "model_allen = NNTransfer(input_dim=sample1_embeddings.shape[1],\n",
    "                   output_dim=len(np.unique(sample1_labels)))\n",
    "model_allen.to(device)  # Move the model to GPU if available\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weight.to(device))\n",
    "optimizer = optim.Adam(model_allen.parameters(), lr=0.001)\n",
    "\n",
    "NNTransferTrain(model_allen, criterion, optimizer, train_loader, val_loader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f0757",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions,sample2_uncertainty = NNTransferPredictWithUncertainty(model_allen, test_loader, device)\n",
    "test_predictions = le.inverse_transform(test_predictions)\n",
    "all_labels = [label.item() for _, label in test_dataset]\n",
    "\n",
    "gt_test_predictions = le.inverse_transform(all_labels)\n",
    "GT_allen_s = gt_test_predictions\n",
    "PRED_allen_s = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426020a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab = pd.crosstab(pd.Series(GT_allen_s, name='Original'),\n",
    "                                pd.Series(PRED_allen_s, name='FuseMap'))\n",
    "\n",
    "cross_tab_normalized = cross_tab.div(cross_tab.sum(axis=0), axis=1)\n",
    "cross_tab_normalized = cross_tab_normalized.div(cross_tab_normalized.sum(axis=1), axis=0)\n",
    "\n",
    "cross_tab_normalized = cross_tab_normalized*100\n",
    "cross_tab_normalized = np.around(cross_tab_normalized)\n",
    "cross_tab_normalized=cross_tab_normalized.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da26a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list= ['01 IT-ET Glut', '02 NP-CT-L6b Glut', \n",
    "            '16 HY MM Glut', '17 MH-LH Glut', '18 TH Glut', '19 MB Glut',\n",
    "          '09 CNU-LGE GABA',  '06 CTX-CGE GABA', '07 CTX-MGE GABA',\n",
    "       '08 CNU-MGE GABA', '20 MB GABA',  '26 P GABA','12 HY GABA',\n",
    "           '10 LSX GABA', '11 CNU-HYa GABA', '14 HY Glut', '15 HY Gnrh1 Glut',\n",
    "           '03 OB-CR Glut',  '13 CNU-HYa Glut','04 DG-IMN Glut','05 OB-IMN GABA', \n",
    "            '28 CB GABA', '29 CB Glut','21 MB Dopa', '22 MB-HB Sero',       \n",
    "        '23 P Glut', '24 MY Glut','27 MY GABA',  '30 Astro-Epen', \n",
    "           '33 Vascular', '34 Immune','31 OPC-Oligo', '32 OEC',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e05ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_normalized = cross_tab_normalized[new_list]\n",
    "\n",
    "cross_tab_normalized = cross_tab_normalized.loc[new_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee1850",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cmap = sns.cubehelix_palette(start=2, rot=0, dark=0, light=1.05, reverse=False, as_cmap=True)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(15,12))\n",
    "ax=sns.heatmap(cross_tab_normalized, cmap=cmap,)\n",
    "plt.title(\"Normalized Correspondence of Two Categories\")\n",
    "# plt.savefig('figures_refine/main_ct_allen.png',dpi=300, transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab4628",
   "metadata": {},
   "source": [
    "transfer to all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4803fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample2_embeddings = ad_fusemap_emb.X\n",
    "dataset2 = TensorDataset(torch.Tensor(sample2_embeddings))\n",
    "dataloader2 = DataLoader(dataset2, batch_size=256, shuffle=False)\n",
    "sample2_predictions,sample2_uncertainty = NNTransferPredictWithUncertainty(model_allen, dataloader2, device)\n",
    "sample2_predictions = le.inverse_transform(sample2_predictions)\n",
    "\n",
    "ad_fusemap_emb.obs['transfer_gt_cell_type_main_Allen'] = sample2_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c960c",
   "metadata": {},
   "source": [
    "### Correspondence between A1N and A2N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8ac8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "label = ad_fusemap_emb.obs['transfer_gt_cell_type_main_STARmap']\n",
    "location = np.array(ad_fusemap_emb.obsm['X_umap'])\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(location, label)\n",
    "\n",
    "querylocation = np.array(ad_fusemap_emb.obsm['X_umap'])\n",
    "\n",
    "predicted_labels = knn.predict(querylocation)\n",
    "ad_fusemap_emb.obs['transfer_gt_cell_type_main_STARmap'] = predicted_labels\n",
    "\n",
    "\n",
    "label = ad_fusemap_emb.obs['transfer_gt_cell_type_main_Allen']\n",
    "location = np.array(ad_fusemap_emb.obsm['X_umap'])\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(location, label)\n",
    "\n",
    "querylocation = np.array(ad_fusemap_emb.obsm['X_umap'])\n",
    "\n",
    "predicted_labels = knn.predict(querylocation)\n",
    "ad_fusemap_emb.obs['transfer_gt_cell_type_main_Allen'] = predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b24bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_starmap_c=np.array(ad_fusemap_emb.obs['transfer_gt_cell_type_main_STARmap'] )\n",
    "PRED_allen_c=np.array(ad_fusemap_emb.obs['transfer_gt_cell_type_main_Allen'])\n",
    "\n",
    "\n",
    "cross_tab = pd.crosstab(pd.Series(GT_starmap_c, name='Original'),\n",
    "                                pd.Series(PRED_allen_c, name='FuseMap'))\n",
    "\n",
    "cross_tab_normalized = cross_tab.div(cross_tab.sum(axis=0), axis=1)\n",
    "cross_tab_normalized = cross_tab_normalized.div(cross_tab_normalized.sum(axis=1), axis=0)\n",
    "\n",
    "cross_tab_normalized = cross_tab_normalized*100\n",
    "cross_tab_normalized = np.around(cross_tab_normalized)\n",
    "cross_tab_normalized=cross_tab_normalized.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ccd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_list=['Telencephalon projecting excitatory neurons', 'Di- and mesencephalon excitatory neurons',\n",
    "          'Telencephalon projecting inhibitory neurons', 'Telencephalon inhibitory interneurons',\n",
    "          'Di- and mesencephalon inhibitory neurons', 'Peptidergic neurons', 'Glutamatergic neuroblasts',\n",
    "          'Dentate gyrus granule neurons','Non-glutamatergic neuroblasts',\n",
    "          'Olfactory inhibitory neurons', 'Cerebellum neurons', 'Cholinergic and monoaminergic neurons',\n",
    "          'Hindbrain neurons/Spinal cord neurons', 'Subcommissural organ hypendymal cells','Ependymal cells', \n",
    "          'Choroid plexus epithelial cells','Astrocytes', \n",
    "          'Vascular smooth muscle cells' , 'Pericytes',   'Vascular endothelial cells', 'Vascular and leptomeningeal cells',\n",
    "          'Perivascular macrophages', 'Microglia', \n",
    "        'Oligodendrocyte precursor cells', 'Oligodendrocytes',\n",
    "        'Olfactory ensheathing cells',]\n",
    "\n",
    "new_list= ['01 IT-ET Glut', '02 NP-CT-L6b Glut', \n",
    "            '16 HY MM Glut', '17 MH-LH Glut', '18 TH Glut', '19 MB Glut',\n",
    "           '09 CNU-LGE GABA',  '06 CTX-CGE GABA', '07 CTX-MGE GABA',\n",
    "           '08 CNU-MGE GABA', '20 MB GABA',  '26 P GABA','12 HY GABA',\n",
    "           '10 LSX GABA', '11 CNU-HYa GABA',\n",
    "            '14 HY Glut', '03 OB-CR Glut',  '13 CNU-HYa Glut',\n",
    "           '04 DG-IMN Glut','05 OB-IMN GABA', \n",
    "            '28 CB GABA', '29 CB Glut','21 MB Dopa', '22 MB-HB Sero',\n",
    "            '23 P Glut', '24 MY Glut','27 MY GABA',  '30 Astro-Epen',\n",
    "            '33 Vascular', '34 Immune','31 OPC-Oligo', '32 OEC',]\n",
    "\n",
    "cross_tab_normalized = cross_tab_normalized[new_list]\n",
    "cross_tab_normalized = cross_tab_normalized.loc[old_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24e4f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cmap = sns.cubehelix_palette(start=2, rot=0, dark=0, light=1.05, reverse=False, as_cmap=True)\n",
    "plt.figure(figsize=(10,6))\n",
    "ax=sns.heatmap(cross_tab_normalized, cmap=cmap,)\n",
    "plt.title(\"Normalized Correspondence of Two Categories\")\n",
    "# plt.savefig('figures_refine/main_ct_corr.png',dpi=300, transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f9a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
